# ğŸ§  HW2: Machine Translation with Transformers

This project implements a German-to-English machine translation model using the Transformer architecture from scratch.

## ğŸ“š Overview

This week, we built a German-English translation model from scratch using the Transformer architecture as part of a graduate-level NLP course at NYU
The project includes theoretical derivations, implementation, training, and analysis.

---

## ğŸ“ Directory Structure

```text
code/
â”œâ”€â”€ layers.py              # Transformer modules (MultiHeadAttention, PositionalEncoding, etc.)
â”œâ”€â”€ main.py                # Entry point for training and evaluation
â”œâ”€â”€ transformer.py         # Transformer model definition (encoder/decoder stack)
â”œâ”€â”€ utils.py               # Beam search, data batching, helper functions
â”œâ”€â”€ test.py                # Unit tests
â”œâ”€â”€ vocab.pt               # Saved vocabulary object
â”œâ”€â”€ requirements.txt       # Required packages
â”œâ”€â”€ out_greedy.txt         # Translation results using greedy decoding
â”œâ”€â”€ out_beam.txt           # Translation results using beam search
â”œâ”€â”€ README.md              # This file
â”œâ”€â”€ data/                  # Preprocessed data (Multi30k dataset)
â””â”€â”€ __pycache__/           # Python cache files
```

---

## ğŸ“¦ Data

- Dataset: [Multi30k (German-English)](https://github.com/multi30k/dataset)
- Preprocessed files are stored in `./data/`
- Vocabulary object is saved as `vocab.pt`

---

## ğŸ”® Maybe next week

- Add subword tokenization (e.g., SentencePiece or Byte Pair Encoding)
- Visualize attention weights for better model interpretability
- Add coverage penalty to improve beam search diversity
- Explore BLEURT or COMET as alternative evaluation metrics

---

## âœï¸ Author

Chenxin Gu @ NYU Data Science  
For academic use only.
